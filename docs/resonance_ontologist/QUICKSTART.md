# Resonance Ontology: QUICKSTART

**A 10-minute visual overview of the complete framework**

---

## The Core Idea

**Identity = Persistence of Spectral Fingerprint**

Everything that exists has a characteristic frequency signature. When that signature persists despite small changes, the entity is *real* and *stable*.

```
System dynamics  â†’  Extract frequencies  â†’  Spectral signature  â†’  Entity identity
(any system)         (via Fourier)           (frequency histogram)    (equivalence class)
```

---

## The Archetype: Orbital Resonance

Start with the **cleanest example**: planets in orbit.

### Pluto & Neptune: A 3:2 Resonance

```
Neptune (slower, outer)  â”
                         â”‚  Every time Neptune completes 2 orbits,
                         â”‚  Pluto completes 3 orbits
Pluto (faster, inner)    â”˜

Result: Fixed phase relationship (resonant angle librates, stays bounded)
Effect: Pluto protected from close encounters with Neptune
```

**The key insight:**
- Integer ratio (3:2) emerges from gravitational dynamics
- Phase locking prevents chaotic collisions
- This is a **hard resonance** (exact integer constraint)

---

## Generalization 1: Spectral Signatures (Any System)

Orbital resonances are too rigid (require exact integer ratios, low dimension, specific physics).

But the *concept* generalizes:

```
ANY SYSTEM with oscillations has a spectral signature (frequency fingerprint).
                              â†“
                   Two systems are "the same" (same entity)
                      if their spectra are close
                              â†“
                  You can do resonance theory without orbits.
```

**Example: Neural network layer**

```
Layer activations over time  â†’  Spectral signature (which frequencies are active)
                              â†’  Compare to other layers/models
                              â†’  Identify entity (this activation pattern = "attention to syntax")
```

---

## The 4-Level Hierarchy

### Level 1: Orbital Resonance
```
Constraints: Exact integer ratios (3:2, 2:1, etc.)
Dimension:  2-6 degrees of freedom
Dynamics:   Hamiltonian (gravity, conservative)
Stability:  MAXIMUM (librarian angel locked forever)
Example:    Pluto-Neptune 3:2
```

### Level 2: Spectral Resonance  
```
Constraints: Approximate ratios (1.50 â‰ˆ 3:2)
Dimension:  Any (1, 10, 1000 dimensions)
Dynamics:   Any (linear, nonlinear, stochastic)
Stability:  ROBUST (modes persist under noise)
Example:    Identifying a person by voice despite background noise
```

### Level 3: Adaptive Resonance
```
Constraints: Integer relations implicit (emerge from learning)
Dimension:  Very high (neural network weights)
Dynamics:   Learning-based (gradient descent)
Stability:  PLASTIC (can restructure via training)
Example:    Multi-task neural network developing task-specific modes
```

### Level 4: General Intelligence
```
Constraints: Self-modifying (no fixed structure)
Dimension:  Grows/shrinks as needed
Dynamics:   Meta-learning (learns how to learn)
Stability:  ADAPTIVE (rewrites itself)
Example:    Theoretical aspiration, not yet achieved
```

---

## The Key Dynamics: Libration vs Circulation

### Libration = Captured (Stable)
```
Phase angle Ï† oscillates around a fixed center

      Ï†(t)
        â†‘     /â€¾\        /â€¾\        /â€¾\
        |    /   \      /   \      /   \
        |___/     \____/     \____/     \___
             â†‘            â†‘            â†‘
        Oscillates around center (BOUNDED)
        
â†’ System is in resonance (captured)
```

### Circulation = Free (Unstable)
```
Phase angle Ï† winds monotonically

      Ï†(t)
        â†‘   
        |         â•±â•±â•±â•±â•±
        |      â•±â•±â•±â•±
        |   â•±â•±â•±â•±
        |â•±â•±â•±â•±
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ t
             â†‘
        Unbounded winding drift (CIRCULATING)
        
â†’ System is escaping resonance (free passage)
```

---

## The Adiabatic Capture Principle

**Core prediction of the framework:**

```
Migration Rate vs Convergence Time
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

For a system approaching a resonance:

   Slow migration    â†’  CAPTURE (gets trapped)
   Fast migration    â†’  PASSAGE (flies through)

Quantitatively:
   Îº = (migration speed Ã— oscillation time) / basin width
   
   Îº << 1  â†’  Capture probable
   Îº >> 1  â†’  Passage probable
```

**Physical analogy**: 

Imagine a ball rolling into a potential well:
- Roll slowly â†’ falls in and settles (captured)
- Roll fast â†’ bounces out and keeps going (passage)

---

## Application to Neural Networks: Grokking

### The Grokking Phenomenon

A neural network trained on modular arithmetic $a + b \pmod{p}$:

```
Loss & Accuracy vs Epoch
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

     Loss
    â”Œâ”€â”€â”€â”€â”
    â”‚  â•²  â•²
    â”‚   â•²  â•²        â†  Pre-grokking: loss decreases, 
    â”‚    â•²  â•²           but test accuracy stays random
    â”‚     â•²  â•²
    â”‚      â•²  â•²â•²â•²â•²â•²â•²â•²â† Grokking transition (sharp phase change)
    â”‚         â•²â•²
    â”‚          â•²
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    Accuracy
    â”Œâ”€â”€â”€â”€â”
    â”‚
    â”‚    â‰ˆ random (~1%)     â•±â•±â•± â† Sudden jump to
    â”‚                    â•±â•±â•±      high accuracy (>99%)
    â”‚                â•±â•±â•±
    â”‚             â•±â•±â•±
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         Pre        Grokking   Post
```

### The Resonance Interpretation

**Pre-grokking**: Circulating regime
- Spectral signature oscillates without settling
- Phase angle drifts (unbounded)
- Loss changes randomly, no convergence
- System migrating through representation space

**Grokking transition**: Capture begins
- Learning rate or data distribution becomes "slower" (Îº drops below threshold)
- System crosses threshold into resonance zone
- Phase angle transitions from circulating to librating

**Post-grokking**: Librating regime
- Spectral signature locks to generalizing mode
- Phase angle librates around fixed center
- Loss decreases smoothly, accuracy saturates
- System captured in stable representation

---

## The Unifying Principle

```
ORBITAL MECHANICS
    â†“ (generalize integer constraints)
SPECTRAL RESONANCE
    â†“ (allow learning)
ADAPTIVE RESONANCE
    â†“ (allow self-modification)
GENERAL INTELLIGENCE
```

**Each level:**
- Adds flexibility
- Loses explainability
- Builds on the previous level

**Why this matters:**
- Orbital resonance gives *perfect* understanding but *zero* flexibility
- General intelligence gives *maximal* flexibility but *zero* understanding
- Real systems live somewhere in between

---

## Three Concrete Examples

### Example 1: Pluto-Neptune (Orbital Level)
```
What: Two planets
Identity: 3:2 period ratio that stays locked
Stability: Over 4 billion years (solar system age)
Measurement: Orbital periods (extremely precise)
Rigidity: Cannot change without external catastrophe
```

### Example 2: Speech Recognition (Spectral Level)
```
What: Human voice saying "hello"
Identity: Characteristic frequencies (formants) unique to speaker
Stability: Same person says it different ways â†’ same spectral signature
Measurement: Spectrogram (frequency vs time heatmap)
Rigidity: Fixed speaker identity, but can adapt to noise
```

### Example 3: Neural Network Persona (Adaptive Level)
```
What: Language model with multiple response modes
Identity: "Analyst" persona (characteristic activation patterns)
Stability: Routes to same persona for related queries
Measurement: Router logits, attention heads
Rigidity: Can learn new personas, merge old ones, restructure
```

---

## The Core Prediction (Testable)

```
Adiabatic Parameter Predicts Learning Speed
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

For any learning system with:
  - migration rate v_mig (how fast target changes)
  - convergence time T_conv (how fast you can reach target)
  - basin width Îµ (tolerance for "same" solution)

Define: Îº = (v_mig Ã— T_conv) / Îµ

Prediction:
  Îº << 1  â†’  Fast learning (captured quickly)
  Îº â‰ˆ 1   â†’  Transition regime (where phase changes happen)
  Îº >> 1  â†’  Slow learning (passes through without settling)

This applies to:
  - Modular arithmetic grokking âœ“ (verified)
  - Multi-task learning (untested)
  - Curriculum learning (untested)
  - Any neural network (untested)
```

---

## Hierarchy Visualization

```
General Intelligence  â– â– â– â– â– â– â– â– â– â–  (Maximal flexibility, zero structure)
                      â–²
                      â”‚ Remove architecture constraints
                      â”‚
Adaptive Resonance    â– â– â– â– â– â– â– â–  â–  (Learn modes & topology)
                      â–²
                      â”‚ Allow mode learning
                      â”‚
Spectral Resonance    â– â– â– â– â– â–  â–  â–  (Fixed modes, flexible physics)
                      â–²
                      â”‚ Relax integer constraints
                      â”‚
Orbital Resonance     â– â–  â–  â–  â–  â–  (Maximal rigidity, perfect understanding)

Legend: â–  = constraint, space = freedom
```

**Key insight**: You can't skip levels. You must relax constraints gradually.

---

## What You Get

### Theory
- âœ… Unified mathematical language for any system with modes
- âœ… Quantitative prediction (Îº criterion) for capture
- âœ… Connection between orbital mechanics and learning
- âœ… Four-level ontology from rigid to flexible

### Practice
- âœ… Algorithm to detect capture in time series
- âœ… Code to compute spectral signatures
- âœ… Formula to predict grokking onset
- âœ… Methods to design curricula

### Understanding
- âœ… Why grokking happens (phase transition)
- âœ… Why multi-task learning is hard (resonance overlap)
- âœ… Why curriculum matters (migration rate)
- âœ… Why neural networks work (soft resonance)

---

## Next Steps

### ğŸ¯ **If you want intuition** (30 min)
1. Run `resonance_ontology_visualization.jsx` (interactive demo)
2. Skim this file + `00_README.md`
3. Done! You understand the picture.

### ğŸ“š **If you want theory** (2 hours)
1. Read `01_orbital_resonances.md` (orbital foundations)
2. Read `02_spectral_signatures.md` (generalization)
3. Read `03-07 Â§ 6` (learning applications)
4. Read `03-07 Â§ 7` (hierarchy and philosophy)

### ğŸ”¬ **If you want implementation** (3 hours)
1. Start with `/resonance_ontology/examples.py`
2. Study `MATHEMATICS_COMPLETE Â§ 8` (algorithms)
3. Adapt to your data
4. Test predictions on your system

### ğŸ“– **If you want everything** (1 day)
See `MASTER_GUIDE.md` for complete learning paths by background

---

## Key Takeaways

| Concept | Intuition |
|---------|-----------|
| **Resonance** | When two things synchronize (planets, frequencies, ideas) |
| **Identity** | What persists despite small changes (your spectral fingerprint) |
| **Libration** | Captured, oscillating stably around a center |
| **Circulation** | Free, drifting unboundedly without settling |
| **Adiabatic** | Slow changes preserve structure; fast changes break it |
| **Hierarchy** | Trade rigidity for flexibility as you go up levels |
| **Grokking** | Transition from circulation to libration in learning |

---

## One-Sentence Summary

**Identity is what persists when a system oscillates around a stable mode under small perturbations, and this principle explains everything from planetary orbits to why neural networks suddenly learn.**

---

## Visual Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           RESONANCE ONTOLOGY: The Big Picture              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  ANY SYSTEM WITH MODES (orbits, networks, etc.)            â”‚
â”‚              â†“                                              â”‚
â”‚       Extract spectral signature (frequencies)             â”‚
â”‚              â†“                                              â”‚
â”‚  Compare to other systems (distance metric)                â”‚
â”‚              â†“                                              â”‚
â”‚  If close â†’ same entity (equivalence class)                â”‚
â”‚              â†“                                              â”‚
â”‚  Track how entity evolves (libration vs circulation)       â”‚
â”‚              â†“                                              â”‚
â”‚  Predict capture with Îº parameter (adiabatic criterion)    â”‚
â”‚              â†“                                              â”‚
â”‚  Understand learning, physics, AI through unified lens     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Ready? Start with the interactive visualization or read `01_orbital_resonances.md` next.**